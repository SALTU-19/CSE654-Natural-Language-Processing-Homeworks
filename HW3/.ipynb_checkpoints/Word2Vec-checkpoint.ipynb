{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include libraries\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from syllable import Encoder\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    " \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate ngrams function\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "\n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "\n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert turkish letters to english letters\n",
    "def turkish_to_english(string):\n",
    "    choices = {\"İ\": \"I\", \"ş\": \"s\", \"Ş\": \"S\", \"ı\": \"i\", \"ö\": \"o\", \"ü\": \"u\", \"ç\": \"c\", \"ğ\": \"g\", \"Ç\": \"C\", \"Ö\": \"O\", \"Ü\": \"U\", \"Ğ\": \"G\", \"â\": \"a\", \"î\": \"i\", \"û\": \"u\", \"ê\": \"e\", \"ô\": \"o\", \"Â\": \"A\", \"Î\": \"I\", \"Û\": \"U\", \"Ê\": \"E\", \"Ô\": \"O\", \"â\": \"a\", \"î\": \"i\", \"û\": \"u\", \"ê\": \"e\", \"ô\": \"o\", \"Â\": \"A\", \"Î\": \"I\", \"Û\": \"U\", \"Ê\": \"E\", \"Ô\": \"O\", \"â\": \"a\", \"î\": \"i\", \"û\": \"u\",\n",
    "               \"ê\": \"e\", \"ô\": \"o\", \"Â\": \"A\", \"Î\": \"I\", \"Û\": \"U\", \"Ê\": \"E\", \"Ô\": \"O\", \"â\": \"a\", \"î\": \"i\", \"û\": \"u\", \"ê\": \"e\", \"ô\": \"o\", \"Â\": \"A\", \"Î\": \"I\", \"Û\": \"U\", \"Ê\": \"E\", \"Ô\": \"O\", \"â\": \"a\", \"î\": \"i\", \"û\": \"u\", \"ê\": \"e\", \"ô\": \"o\", \"Â\": \"A\", \"Î\": \"I\", \"Û\": \"U\", \"Ê\": \"E\", \"Ô\": \"O\", \"â\": \"a\", \"î\": \"i\", \"û\": \"u\", \"ê\": \"e\", \"ô\": \"o\", \"Â\": \"A\", \"Î\": \"I\", \"Û\": \"U\"}\n",
    "    for i in range(len(string)):\n",
    "        string = string.replace(\n",
    "            string[i:i+1], choices.get(string[i], string[i]))\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse string into syllables\n",
    "consonant = [\"b\", \"c\", \"d\", \"g\", \"ğ\", \"j\", \"l\", \"m\", \"n\", \"r\",\n",
    "             \"v\", \"y\", \"z\", \"ç\", \"f\", \"h\", \"k\", \"p\", \"s\", \"ş\", \"t\"]\n",
    "vowel = [\"a\", \"ı\", \"o\", \"u\", \"e\", \"i\", \"ö\", \"ü\"]\n",
    "\n",
    "# params chosen for demonstration purposes\n",
    "encoder = Encoder(lang=\"tr\", limitby=\"vocabulary\", limit=3000)\n",
    "\n",
    "def parse_syllable(string):\n",
    "    string = turkish_to_english(string)\n",
    "    return encoder.tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_two(string):\n",
    "    return string.split(\" \")[1]\n",
    "def parse_string_three(string):\n",
    "    return string.split(\" \")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_element_matrix(sparse_matrix, element):\n",
    "    count = 0\n",
    "    for i in sparse_matrix.data:\n",
    "        if(i == element):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turing_smooting(ngram_matrix, ngrams, unique_ngrams):\n",
    "    gt_smooth = np.zeros((len(unique_ngrams), len(unique_ngrams)))\n",
    "    sparse_matrix = csc_matrix(ngram_matrix)\n",
    "    count_one = count_element_matrix(sparse_matrix, 1)\n",
    "    # calculate good turing smoothing\n",
    "    for i in range(len(ngram_matrix)):\n",
    "        for j in range(len(ngram_matrix[i])):\n",
    "            if(ngram_matrix[i][j] == 0):\n",
    "                gt_smooth[i][j] = count_one / len(ngrams)\n",
    "            else:\n",
    "                gt_smooth[i][j] = (ngram_matrix[i][j]+1) * \\\n",
    "                    count_element_matrix(sparse_matrix, ngram_matrix[i][j]+1) / \\\n",
    "                    count_element_matrix(sparse_matrix, ngram_matrix[i][j])\n",
    "    return gt_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigram_matrix(unique_bigrams, bigrams):\n",
    "    bigram_matrix = np.zeros((len(unique_bigrams), len(unique_bigrams)))\n",
    "    for i in range(len(bigrams)-1):\n",
    "        bigram_matrix[unique_bigrams.index(\n",
    "            bigrams[i])][unique_bigrams.index(bigrams[i+1])] += 1\n",
    "    gt_smooth = good_turing_smooting(bigram_matrix, bigrams, unique_bigrams)\n",
    "    return gt_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_towgram_matrix(unique_towgrams, towgrams, unique_bigrams):\n",
    "    towgram_matrix = np.zeros((len(unique_towgrams), len(unique_bigrams)))\n",
    "    for i in range(len(towgrams)-1):\n",
    "        towgram_matrix[unique_towgrams.index(\n",
    "            towgrams[i])][unique_bigrams.index(parse_string_two(towgrams[i+1]))] += 1\n",
    "\n",
    "    gt_smooth = good_turing_smooting(towgram_matrix, towgrams, unique_towgrams)\n",
    "    return gt_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_threegram_matrix(unique_threegrams, threegrams, unique_bigrams):\n",
    "    threegram_matrix = np.zeros((len(unique_threegrams), len(unique_bigrams)))\n",
    "    for i in range(len(threegrams)-1):\n",
    "        threegram_matrix[unique_threegrams.index(\n",
    "            threegrams[i])][unique_bigrams.index(parse_string_three(threegrams[i+1]))] += 1\n",
    "\n",
    "    gt_smoothing = good_turing_smooting(\n",
    "        threegram_matrix, threegrams, unique_threegrams)\n",
    "    return gt_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Reads file\n",
    "sample = open(\"text3.txt\", mode=\"r\", encoding=\"utf-8\")\n",
    "s = sample.read()\n",
    "\n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_sentences = []\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        parsed_words = parse_syllable(j.lower())\n",
    "        unigram = generate_ngrams(parsed_words, 1)\n",
    "        for i in unigram:\n",
    "            temp.append(i)\n",
    "\n",
    "    uni_sentences.append(temp)\n",
    "    \n",
    "\n",
    "unigram_w2v_model = Word2Vec(min_count=20,\n",
    "                 window=2,\n",
    "                 sample=6e-5, \n",
    "                 alpha=0.03, \n",
    "                 min_alpha=0.0007, \n",
    "                 negative=20,\n",
    "                 workers=cores-1)\n",
    "unigram_w2v_model.build_vocab(uni_sentences, progress_per=10000)\n",
    "\n",
    "unigram_w2v_model.train(uni_sentences, total_examples=unigram_w2v_model.corpus_count, epochs=30, report_delay=1)   \n",
    "\n",
    "unigram_w2v_model.init_sims(replace=True)   \n",
    "unigram_w2v_model.wv.save_word2vec_format('model1.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sentences = []\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        parsed_words = parse_syllable(j.lower())\n",
    "        twogram = generate_ngrams(parsed_words, 2)\n",
    "        for i in twogram:\n",
    "            temp.append(i)\n",
    "\n",
    "    two_sentences.append(temp)\n",
    "\n",
    "twogram_w2v_model = Word2Vec(min_count=20,\n",
    "                 window=2,\n",
    "                 sample=6e-5, \n",
    "                 alpha=0.03, \n",
    "                 min_alpha=0.0007, \n",
    "                 negative=20,\n",
    "                 workers=cores-1)\n",
    "twogram_w2v_model.build_vocab(two_sentences, progress_per=10000)\n",
    "\n",
    "twogram_w2v_model.train(two_sentences, total_examples=twogram_w2v_model.corpus_count, epochs=30, report_delay=1)   \n",
    "\n",
    "twogram_w2v_model.init_sims(replace=True)   \n",
    "twogram_w2v_model.wv.save_word2vec_format('model2.txt', binary=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_sentences = []\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        parsed_words = parse_syllable(j.lower())\n",
    "        threegram = generate_ngrams(parsed_words, 3)\n",
    "        for i in threegram:\n",
    "            temp.append(i)\n",
    "\n",
    "    three_sentences.append(temp)\n",
    "    \n",
    "threegram_w2v_model = Word2Vec(min_count=20,\n",
    "                 window=2,\n",
    "                 sample=6e-5, \n",
    "                 alpha=0.03, \n",
    "                 min_alpha=0.0007, \n",
    "                 negative=20,\n",
    "                 workers=cores-1)\n",
    "threegram_w2v_model.build_vocab(three_sentences, progress_per=10000)\n",
    "\n",
    "threegram_w2v_model.train(three_sentences, total_examples=threegram_w2v_model.corpus_count, epochs=30, report_delay=1)   \n",
    "\n",
    "threegram_w2v_model.init_sims(replace=True)\n",
    "threegram_w2v_model.wv.save_word2vec_format('model3.txt', binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_most_similar_unigram():\n",
    "    print(\"Most similar 1-gram words of 'ri': \")\n",
    "    print(unigram_w2v_model.wv.most_similar(positive=[\"ri\"]))\n",
    "def  test_most_similar_twogram():\n",
    "    print(\"Most similar 2-gram words of 'le ri': \")\n",
    "    print(twogram_w2v_model.wv.most_similar(positive=[\"le ri\"]))\n",
    "def  test_most_similar_threegram():\n",
    "    print(\"Most similar 3-gram words of 'le ri ni': \")\n",
    "    print(threegram_w2v_model.wv.most_similar(positive=[\"le ri ni\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_morf_analogy_unigram():\n",
    "    print(\"Similarity between 'ri' and 'rin' for 1-gram words: \")\n",
    "    print(unigram_w2v_model.wv.similarity(\"le\", 'len'))\n",
    "    print(\"Similarity between 'ni' and 'nin' for 1-gram words: \")\n",
    "    print(unigram_w2v_model.wv.similarity(\"la\", 'lan'))\n",
    "def  test_morf_analogy_twogram():\n",
    "    print(\"Similarity between 'le ri' and 'le rin' for 2-gram words: \")\n",
    "    print(twogram_w2v_model.wv.similarity(\"le ri\", 'le rin'))\n",
    "    print(\"Similarity between 'la ri' and 'la rin' for 2-gram words: \")\n",
    "    print(twogram_w2v_model.wv.similarity(\"la ri\", 'la rin'))\n",
    "def  test_morf_analogy_threegram():\n",
    "    print(\"Similarity between 'le ri ni' and 'le ri nin' for 3-gram words: \")\n",
    "    print(threegram_w2v_model.wv.similarity(\"le ri ni\", 'le ri nin'))\n",
    "    print(\"Similarity between 'la ri ni' and 'la ri nin' for 3-gram words: \")\n",
    "    print(threegram_w2v_model.wv.similarity(\"la ri ni\", 'la ri nin')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar 1-gram words of 'ri': \n",
      "[('rin', 0.5653178691864014), ('riy', 0.5601409077644348), ('ve', 0.5072356462478638), ('nin', 0.46980857849121094), ('ler', 0.4193567633628845), ('le', 0.3885158896446228), ('ni', 0.37572386860847473), ('ye', 0.36647459864616394), ('tum', 0.3591952919960022), ('ci', 0.35564112663269043)]\n",
      "-------------------\n",
      "Most similar 1-gram words of 'le ri': \n",
      "[('le rin', 0.7904993295669556), ('le re', 0.7496670484542847), ('ri ni', 0.723483681678772), ('ri ne', 0.6991896629333496), ('ri nin', 0.6886616945266724), ('le riy', 0.6594251394271851), ('rin den', 0.6292649507522583), ('rin de', 0.577063262462616), ('riy le', 0.5619356632232666), ('di ger', 0.4796064496040344)]\n",
      "-------------------\n",
      "Most similar 1-gram words of 'le ri ni': \n",
      "[('le ri nin', 0.6961844563484192), ('le ri ne', 0.6721827983856201), ('le ri dir', 0.5485888719558716), ('le riy le', 0.465102881193161), ('nim le ri', 0.45228278636932373), ('la ri ni', 0.44923579692840576), ('et me le', 0.4411030411720276), ('on la ri', 0.43123990297317505), ('le dik le', 0.42211806774139404), ('me dik le', 0.4211006462574005)]\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "#call tests of most similar\n",
    "test_most_similar_unigram()\n",
    "print(\"-------------------\")\n",
    "test_most_similar_twogram()\n",
    "print(\"-------------------\")\n",
    "test_most_similar_threegram()\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'ri' and 'rin' for 1-gram words: \n",
      "0.4380952\n",
      "Similarity between 'ni' and 'nin' for 1-gram words: \n",
      "0.55822575\n",
      "-------------------\n",
      "Similarity between 'le ri' and 'le rin' for 2-gram words: \n",
      "0.7904993\n",
      "Similarity between 'la ri' and 'la rin' for 2-gram words: \n",
      "0.80741656\n",
      "-------------------\n",
      "Similarity between 'le ri ni' and 'le ri nin' for 3-gram words: \n",
      "0.69618446\n",
      "Similarity between 'la ri ni' and 'la ri nin' for 3-gram words: \n",
      "0.7011584\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "#call tests of morphlogy analogy\n",
    "test_morf_analogy_unigram()\n",
    "print(\"-------------------\")\n",
    "test_morf_analogy_twogram()\n",
    "print(\"-------------------\")\n",
    "test_morf_analogy_threegram()\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
